{
  "nbformat": 4,
  "nbformat_minor": 0,
  "metadata": {
    "colab": {
      "provenance": []
    },
    "kernelspec": {
      "name": "python3",
      "display_name": "Python 3"
    },
    "language_info": {
      "name": "python"
    }
  },
  "cells": [
    {
      "cell_type": "markdown",
      "source": [
        "#Boosting Techniques"
      ],
      "metadata": {
        "id": "wxfB6mMT8dKE"
      }
    },
    {
      "cell_type": "markdown",
      "source": [
        "\n",
        "---\n",
        "\n",
        "**1. What is Boosting in Machine Learning?**\n",
        "\n",
        "Boosting is an ensemble technique that combines multiple weak learners sequentially to form a strong learner. Each model tries to correct the errors of its predecessor, resulting in improved overall performance and reduced bias.\n",
        "\n",
        "---\n",
        "\n",
        "\n",
        "**2. How does Boosting differ from Bagging?**\n",
        "\n",
        "Bagging builds multiple models independently and averages their results to reduce variance, while Boosting builds models sequentially, focusing on the mistakes of previous ones, aiming to reduce both bias and variance.\n",
        "\n",
        "---\n",
        "\n",
        "**3. What is the key idea behind AdaBoost?**\n",
        "\n",
        "AdaBoost focuses on misclassified data points by assigning higher weights to them in each round. It combines weak learners into a strong model, improving classification by emphasizing difficult examples.\n",
        "\n",
        "---\n",
        "\n",
        "**4. Explain the working of AdaBoost with an example.**\n",
        "\n",
        "AdaBoost starts with equal weights for all samples. After each weak learner (like a decision stump), weights of misclassified points are increased. The final prediction is a weighted sum of all learners. For example, classifying spam emails by improving on previous misclassifications.\n",
        "\n",
        "---\n",
        "\n",
        "**5. What is Gradient Boosting, and how is it different from AdaBoost?**\n",
        "\n",
        "Gradient Boosting optimizes a loss function by adding models that predict residual errors. Unlike AdaBoost, it uses gradient descent to minimize the error and is more flexible with loss functions and regularization.\n",
        "\n",
        "---\n",
        "\n",
        "**6. What is the loss function in Gradient Boosting?**\n",
        "\n",
        "The loss function in Gradient Boosting measures how well the model predicts outcomes. Common ones include mean squared error for regression and log loss for classification. The model minimizes this loss through gradient descent.\n",
        "\n",
        "---\n",
        "\n",
        "**7. How does XGBoost improve over traditional Gradient Boosting?**\n",
        "\n",
        "XGBoost improves speed and performance using regularization, parallel processing, tree pruning, and handling of missing values. It’s designed for scalability and efficiency in large-scale datasets.\n",
        "\n",
        "---\n",
        "\n",
        "**8. What is the difference between XGBoost and CatBoost?**\n",
        "\n",
        "XGBoost is powerful but needs preprocessing for categorical data. CatBoost, however, handles categorical variables internally, avoiding manual encoding and reducing overfitting with ordered boosting and better default hyperparameters.\n",
        "\n",
        "---\n",
        "\n",
        "**9. What are some real-world applications of Boosting techniques?**\n",
        "\n",
        "Boosting is widely used in fraud detection, customer churn prediction, image classification, ranking in search engines, and disease diagnosis due to its high accuracy and performance.\n",
        "\n",
        "---\n",
        "\n",
        "**10. How does regularization help in XGBoost?**\n",
        "\n",
        "Regularization in XGBoost penalizes model complexity by adding L1 (lasso) or L2 (ridge) penalties. This reduces overfitting, improves generalization, and stabilizes model performance on unseen data.\n",
        "\n",
        "---\n",
        "\n",
        "**11. What are some hyperparameters to tune in Gradient Boosting models?**\n",
        "\n",
        "Important hyperparameters include learning rate, number of estimators (trees), max depth, min samples split, subsample ratio, and regularization parameters. Tuning these controls model complexity and performance.\n",
        "\n",
        "---\n",
        "\n",
        "**12. What is the concept of Feature Importance in Boosting?**\n",
        "\n",
        "Feature importance indicates how much each feature contributes to the model’s predictions. Boosting models compute it by evaluating how often a feature is used for splitting and its impact on reducing error.\n",
        "\n",
        "---\n",
        "\n",
        "**13. Why is CatBoost efficient for categorical data?**\n",
        "\n",
        "CatBoost efficiently handles categorical variables without preprocessing by using ordered boosting and target statistics. It avoids overfitting and preserves feature information, making it ideal for datasets with many categorical features.\n",
        "\n",
        "\n"
      ],
      "metadata": {
        "id": "8Xe-2cq68f3z"
      }
    },
    {
      "cell_type": "markdown",
      "source": [
        "#Practical"
      ],
      "metadata": {
        "id": "3jA4tDcj-E0I"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "# 14 Train an AdaBoost Classifier on a sample dataset and print model accuracy\n",
        "from sklearn.ensemble import AdaBoostClassifier\n",
        "from sklearn.datasets import load_iris\n",
        "from sklearn.model_selection import train_test_split\n",
        "from sklearn.metrics import accuracy_score\n",
        "\n",
        "X, y = load_iris(return_X_y=True)\n",
        "X_train, X_test, y_train, y_test = train_test_split(X, y, test_size=0.3)\n",
        "\n",
        "model = AdaBoostClassifier()\n",
        "model.fit(X_train, y_train)\n",
        "y_pred = model.predict(X_test)\n",
        "print(\"Accuracy:\", accuracy_score(y_test, y_pred))"
      ],
      "metadata": {
        "id": "S2qpQ55U-LfM"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "# 15 Train an AdaBoost Regressor and evaluate performance using Mean Absolute Error (MAE)\n",
        "from sklearn.ensemble import AdaBoostRegressor\n",
        "from sklearn.datasets import make_regression\n",
        "from sklearn.metrics import mean_absolute_error\n",
        "\n",
        "X, y = make_regression(n_samples=100, n_features=4, noise=0.1)\n",
        "X_train, X_test, y_train, y_test = train_test_split(X, y, test_size=0.3)\n",
        "\n",
        "reg = AdaBoostRegressor()\n",
        "reg.fit(X_train, y_train)\n",
        "y_pred = reg.predict(X_test)\n",
        "print(\"MAE:\", mean_absolute_error(y_test, y_pred))"
      ],
      "metadata": {
        "id": "ilbg15y4-OKe"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "# 16 Train a Gradient Boosting Classifier on the Breast Cancer dataset and print feature importance\n",
        "from sklearn.ensemble import GradientBoostingClassifier\n",
        "from sklearn.datasets import load_breast_cancer\n",
        "\n",
        "X, y = load_breast_cancer(return_X_y=True)\n",
        "X_train, X_test, y_train, y_test = train_test_split(X, y, test_size=0.3)\n",
        "\n",
        "model = GradientBoostingClassifier()\n",
        "model.fit(X_train, y_train)\n",
        "print(\"Feature Importance:\", model.feature_importances_)"
      ],
      "metadata": {
        "id": "eF30rSBe-QfZ"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "# 17 Train a Gradient Boosting Regressor and evaluate using R-Squared Score\n",
        "from sklearn.ensemble import GradientBoostingRegressor\n",
        "from sklearn.metrics import r2_score\n",
        "\n",
        "X, y = make_regression(n_samples=100, n_features=4, noise=0.2)\n",
        "X_train, X_test, y_train, y_test = train_test_split(X, y, test_size=0.3)\n",
        "\n",
        "reg = GradientBoostingRegressor()\n",
        "reg.fit(X_train, y_train)\n",
        "y_pred = reg.predict(X_test)\n",
        "print(\"R2 Score:\", r2_score(y_test, y_pred))\n"
      ],
      "metadata": {
        "id": "anhRsyFA-So8"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "# 18 Train an XGBoost Classifier on a dataset and compare accuracy with Gradient Boosting\n",
        "from xgboost import XGBClassifier\n",
        "from sklearn.metrics import accuracy_score\n",
        "\n",
        "xgb = XGBClassifier()\n",
        "xgb.fit(X_train, y_train)\n",
        "xgb_pred = xgb.predict(X_test)\n",
        "print(\"XGBoost Accuracy:\", accuracy_score(y_test, xgb_pred))\n",
        "\n",
        "gb = GradientBoostingClassifier()\n",
        "gb.fit(X_train, y_train)\n",
        "gb_pred = gb.predict(X_test)\n",
        "print(\"Gradient Boosting Accuracy:\", accuracy_score(y_test, gb_pred))\n"
      ],
      "metadata": {
        "id": "vM4oiAu6-UwA"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "# 19 Train a CatBoost Classifier and evaluate using F1-Score\n",
        "from catboost import CatBoostClassifier\n",
        "from sklearn.metrics import f1_score\n",
        "\n",
        "model = CatBoostClassifier(verbose=0)\n",
        "model.fit(X_train, y_train)\n",
        "y_pred = model.predict(X_test)\n",
        "print(\"F1 Score:\", f1_score(y_test, y_pred))\n"
      ],
      "metadata": {
        "id": "xnFhHg16-X5F"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "# 20 Train an XGBoost Regressor and evaluate using Mean Squared Error (MSE)\n",
        "from xgboost import XGBRegressor\n",
        "from sklearn.metrics import mean_squared_error\n",
        "\n",
        "xgb_reg = XGBRegressor()\n",
        "xgb_reg.fit(X_train, y_train)\n",
        "y_pred = xgb_reg.predict(X_test)\n",
        "print(\"MSE:\", mean_squared_error(y_test, y_pred))\n"
      ],
      "metadata": {
        "id": "wUbdcZDF-aFI"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "# 21 Train an AdaBoost Classifier and visualize feature importance\n",
        "import matplotlib.pyplot as plt\n",
        "\n",
        "model = AdaBoostClassifier()\n",
        "model.fit(X_train, y_train)\n",
        "plt.bar(range(X.shape[1]), model.feature_importances_)\n",
        "plt.title(\"Feature Importance\")\n",
        "plt.show()"
      ],
      "metadata": {
        "id": "QLweYaMc-cJt"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "# 22 Train a Gradient Boosting Regressor and plot learning curves\n",
        "from sklearn.model_selection import learning_curve\n",
        "import numpy as np\n",
        "\n",
        "train_sizes, train_scores, test_scores = learning_curve(GradientBoostingRegressor(), X, y)\n",
        "plt.plot(train_sizes, np.mean(train_scores, axis=1), label=\"Train\")\n",
        "plt.plot(train_sizes, np.mean(test_scores, axis=1), label=\"Test\")\n",
        "plt.legend()\n",
        "plt.title(\"Learning Curves\")\n",
        "plt.show()\n"
      ],
      "metadata": {
        "id": "5lnjZEf--eX4"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "# 23 Train an XGBoost Classifier and visualize feature importance\n",
        "xgb = XGBClassifier()\n",
        "xgb.fit(X_train, y_train)\n",
        "xgb.plot_importance()\n",
        "plt.show()\n"
      ],
      "metadata": {
        "id": "B4QS7Dkt-g_p"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "# 24 Train a CatBoost Classifier and plot the confusion matrix\n",
        "from sklearn.metrics import confusion_matrix, ConfusionMatrixDisplay\n",
        "\n",
        "model = CatBoostClassifier(verbose=0)\n",
        "model.fit(X_train, y_train)\n",
        "y_pred = model.predict(X_test)\n",
        "cm = confusion_matrix(y_test, y_pred)\n",
        "ConfusionMatrixDisplay(cm).plot()\n",
        "plt.show()"
      ],
      "metadata": {
        "id": "stUtutQS-i-V"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "# 25 Train an AdaBoost Classifier with different numbers of estimators and compare accuracy\n",
        "for n in [10, 50, 100]:\n",
        "    model = AdaBoostClassifier(n_estimators=n)\n",
        "    model.fit(X_train, y_train)\n",
        "    print(f\"n_estimators={n}: Accuracy={accuracy_score(y_test, model.predict(X_test))}\")\n"
      ],
      "metadata": {
        "id": "YB74mtUx-kc9"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "# 26 Train a Gradient Boosting Classifier and visualize the ROC curve\n",
        "from sklearn.metrics import roc_curve, RocCurveDisplay\n",
        "\n",
        "model = GradientBoostingClassifier()\n",
        "model.fit(X_train, y_train)\n",
        "y_prob = model.predict_proba(X_test)[:,1]\n",
        "fpr, tpr, _ = roc_curve(y_test, y_prob)\n",
        "RocCurveDisplay(fpr=fpr, tpr=tpr).plot()\n",
        "plt.show()"
      ],
      "metadata": {
        "id": "NKeaLRfK-mfx"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "\n",
        "# 27 Train an XGBoost Regressor and tune the learning rate using GridSearchCV\n",
        "from sklearn.model_selection import GridSearchCV\n",
        "\n",
        "params = {'learning_rate': [0.01, 0.1, 0.2]}\n",
        "grid = GridSearchCV(XGBRegressor(), params, cv=3)\n",
        "grid.fit(X_train, y_train)\n",
        "print(\"Best Params:\", grid.best_params_)\n"
      ],
      "metadata": {
        "id": "xqsHDXfm-opo"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "# 28 Train a CatBoost Classifier on an imbalanced dataset and compare performance with class weighting\n",
        "from sklearn.datasets import make_classification\n",
        "\n",
        "X, y = make_classification(n_samples=500, n_classes=2, weights=[0.9, 0.1], n_features=4)\n",
        "model = CatBoostClassifier(class_weights=[1, 10], verbose=0)\n",
        "model.fit(X, y)\n",
        "print(\"Accuracy:\", model.score(X, y))\n"
      ],
      "metadata": {
        "id": "YJfSTWpC-qtQ"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "# 29 Train an AdaBoost Classifier and analyze the effect of different learning rates\n",
        "for lr in [0.01, 0.1, 1.0]:\n",
        "    model = AdaBoostClassifier(learning_rate=lr)\n",
        "    model.fit(X_train, y_train)\n",
        "    print(f\"Learning Rate={lr}: Accuracy={accuracy_score(y_test, model.predict(X_test))}\")\n"
      ],
      "metadata": {
        "id": "2PwYSzjg-s_s"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "# 30 Train an XGBoost Classifier for multi-class classification and evaluate using log-loss\n",
        "from sklearn.metrics import log_loss\n",
        "\n",
        "X, y = load_iris(return_X_y=True)\n",
        "X_train, X_test, y_train, y_test = train_test_split(X, y)\n",
        "model = XGBClassifier()\n",
        "model.fit(X_train, y_train)\n",
        "y_proba = model.predict_proba(X_test)\n",
        "print(\"Log Loss:\", log_loss(y_test, y_proba))"
      ],
      "metadata": {
        "id": "F9pbhnq8-u8J"
      },
      "execution_count": null,
      "outputs": []
    }
  ]
}